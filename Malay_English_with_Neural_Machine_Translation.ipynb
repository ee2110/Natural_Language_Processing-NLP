{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Malay_English_with_Neural_Machine_Translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZlvuw19TGDA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "c818c607-4732-46b6-bbdc-64fa85e3c5e5"
      },
      "source": [
        "!wget https://github.com/huseinzol05/Malaya-Dataset/raw/master/english-malay/english-malay1.json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-11 09:04:25--  https://github.com/huseinzol05/Malaya-Dataset/raw/master/english-malay/english-malay1.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/english-malay/english-malay1.json [following]\n",
            "--2019-09-11 09:04:26--  https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/english-malay/english-malay1.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4379510 (4.2M) [text/plain]\n",
            "Saving to: ‘english-malay1.json.1’\n",
            "\n",
            "english-malay1.json 100%[===================>]   4.18M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-09-11 09:04:28 (35.5 MB/s) - ‘english-malay1.json.1’ saved [4379510/4379510]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MTvOQD8gDeP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ce1dff5-d42f-4c23-8975-de0e379d121a"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('english-malay1.json') as fopen:\n",
        "  malay_english = json.load(fopen)\n",
        "len(malay_english)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-S14zk6gFrA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f593edac-c754-416f-b239-2ba67b90a9ae"
      },
      "source": [
        "malay_english[:3]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Rachel Pike : The science behind a climate headline',\n",
              "  'rachel pike: sains di sebalik tajuk iklim'],\n",
              " ['That report was written by 620 scientists from 40 countries .',\n",
              "  'laporan itu ditulis oleh 620 saintis dari 40 negara.'],\n",
              " ['They wrote almost a thousand pages on the topic .',\n",
              "  'mereka menulis hampir seribu muka surat mengenai topik itu.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHlw0JmrgKHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "82cf59db-4713-4c05-9d0f-67ef5c5fb979"
      },
      "source": [
        "!pip3 install malaya\n",
        "!pip3 install fuzzywuzzy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: malaya in /usr/local/lib/python3.6/dist-packages (2.7.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from malaya) (2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from malaya) (0.21.3)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from malaya) (1.1.1)\n",
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (from malaya) (1.0.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from malaya) (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from malaya) (1.16.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from malaya) (0.1.83)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from malaya) (1.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from malaya) (2.21.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from malaya) (5.6)\n",
            "Requirement already satisfied: PySastrawi in /usr/local/lib/python3.6/dist-packages (from malaya) (1.2.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from malaya) (0.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from malaya) (1.14.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->malaya) (4.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->malaya) (0.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow->malaya) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->malaya) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->malaya) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->malaya) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->malaya) (1.24.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->malaya) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (0.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->malaya) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->malaya) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow->malaya) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow->malaya) (0.15.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow->malaya) (3.1.1)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.6/dist-packages (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rVeDc6ZgMh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1f94e5e7-a40f-4e9c-b1ba-cc29a24b1251"
      },
      "source": [
        "import malaya\n",
        "import re\n",
        "\n",
        "tokenizer = malaya.preprocessing.SocialTokenizer().tokenize\n",
        "\n",
        "def is_number_regex(s):\n",
        "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
        "        return s.isdigit()\n",
        "    return True\n",
        "\n",
        "def preprocessing(string):\n",
        "    tokenized = tokenizer(string)\n",
        "    tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
        "    return ' '.join(tokenized)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFKaF1cIgO7N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a208ca3c-db4e-41ea-97b3-cbadd472d94f"
      },
      "source": [
        "preprocessing('That report was written by 620 scientists from 40 countries .')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That report was written by <NUM> scientists from <NUM> countries .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv5slZs8gZ-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import json\n",
        "\n",
        "def build_dataset(words, n_words, atleast=1):\n",
        "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
        "    counter = collections.Counter(words).most_common(n_words)\n",
        "    counter = [i for i in counter if i[1] >= atleast]\n",
        "    count.extend(counter)\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return dictionary, reversed_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIldPBGeg_hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english, malay = [], []\n",
        "for row in malay_english:\n",
        "  if len(row[0]) and len(row[1]):\n",
        "    english.append(row[0])\n",
        "    malay.append(row[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HMhPMq4hCA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e0114cf-e6ec-40e4-c2cc-cdcf55a93121"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(range(len(english))):\n",
        "  english[i] = preprocessing(english[i])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19979/19979 [00:02<00:00, 7952.65it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI86bJQ6hEjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1ad1fb9-34e0-43d5-dc30-efc885db0c1e"
      },
      "source": [
        "for i in tqdm(range(len(malay))):\n",
        "  malay[i] = preprocessing(malay[i])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19979/19979 [00:02<00:00, 8608.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SaKqJZuhGjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "concat = ' '.join(english).split()\n",
        "size_concat = len(set(concat))\n",
        "english_dictionary, english_reversed_dictionary = build_dataset(concat, size_concat, atleast=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXeXGFNahIfn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76d235ca-37fd-4eb7-f3a4-6d618c49e68b"
      },
      "source": [
        "len(english_dictionary)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19138"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWoQUy5VhKI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "concat = ' '.join(malay).split()\n",
        "size_concat = len(set(concat))\n",
        "malay_dictionary, malay_reversed_dictionary = build_dataset(concat, size_concat, atleast=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UknINkO9hMNw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6dde8fd8-49d0-4f8e-c044-6a8f4b9b285f"
      },
      "source": [
        "len(malay_dictionary)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13877"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IGGweTThNwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GO = malay_dictionary['GO']\n",
        "PAD = malay_dictionary['PAD']\n",
        "EOS = malay_dictionary['EOS']\n",
        "UNK = malay_dictionary['UNK']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYRUA4_KhPTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69a9825b-dbe6-40cd-c2d1-b5eb275147fd"
      },
      "source": [
        "english[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rachel Pike : The science behind a climate headline'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za7U7SoihR8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "627c9f52-a5b9-4b44-e6eb-5434a11f53ec"
      },
      "source": [
        "for i in range(len(english)):\n",
        "    english[i] += ' EOS'\n",
        "    \n",
        "english[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rachel Pike : The science behind a climate headline EOS'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB6GBNEihTsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Translator:\n",
        "    def __init__(self, size_layer, num_layers, embedded_size,\n",
        "                 from_dict_size, to_dict_size, learning_rate, batch_size):\n",
        "        \n",
        "        def cells(reuse=False):\n",
        "            return tf.nn.rnn_cell.LSTMCell(size_layer,\n",
        "                                           initializer=tf.orthogonal_initializer(),\n",
        "                                           reuse=reuse)\n",
        "          \n",
        "        def attention(encoder_out, seq_len, reuse=False):\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units = size_layer, \n",
        "                                                                    memory = encoder_out,\n",
        "                                                                    memory_sequence_length = seq_len)\n",
        "            return tf.contrib.seq2seq.AttentionWrapper(\n",
        "            cell = tf.nn.rnn_cell.MultiRNNCell([cells(reuse) for _ in range(num_layers)]), \n",
        "                attention_mechanism = attention_mechanism,\n",
        "                attention_layer_size = size_layer)\n",
        "        \n",
        "        self.X = tf.placeholder(tf.int32, [None, None])\n",
        "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
        "        self.X_seq_len = tf.count_nonzero(self.X, 1, dtype=tf.int32)\n",
        "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
        "        batch_size = tf.shape(self.X)[0]\n",
        "        \n",
        "        encoder_embedding = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))\n",
        "        decoder_embedding = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n",
        "        \n",
        "        encoder_out, encoder_state = tf.nn.dynamic_rnn(\n",
        "            cell = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)]), \n",
        "            inputs = tf.nn.embedding_lookup(encoder_embedding, self.X),\n",
        "            sequence_length = self.X_seq_len,\n",
        "            dtype = tf.float32)\n",
        "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
        "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
        "        dense = tf.layers.Dense(to_dict_size)\n",
        "        decoder_cells = attention(encoder_out, self.X_seq_len)\n",
        "        \n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
        "                inputs = tf.nn.embedding_lookup(decoder_embedding, decoder_input),\n",
        "                sequence_length = self.Y_seq_len,\n",
        "                time_major = False)\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                cell = decoder_cells,\n",
        "                helper = training_helper,\n",
        "                initial_state = decoder_cells.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),\n",
        "            \n",
        "                output_layer = dense)\n",
        "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                decoder = training_decoder,\n",
        "                impute_finished = True,\n",
        "                maximum_iterations = tf.reduce_max(self.Y_seq_len))\n",
        "        self.training_logits = training_decoder_output.rnn_output\n",
        "        \n",
        "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "                embedding = decoder_embedding,\n",
        "                start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
        "                end_token = EOS)\n",
        "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                cell = decoder_cells,\n",
        "                helper = predicting_helper,\n",
        "                initial_state = decoder_cells.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),\n",
        "                output_layer = dense)\n",
        "        \n",
        "        predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                decoder = predicting_decoder,\n",
        "                impute_finished = True,\n",
        "                maximum_iterations = tf.reduce_max(self.X_seq_len))\n",
        "        self.predicting_ids = predicting_decoder_output.sample_id\n",
        "        \n",
        "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
        "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
        "                                                     targets = self.Y,\n",
        "                                                     weights = masks)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
        "        y_t = tf.argmax(self.training_logits,axis=2)\n",
        "        y_t = tf.cast(y_t, tf.int32)\n",
        "        self.prediction = tf.boolean_mask(y_t, masks)\n",
        "        mask_label = tf.boolean_mask(self.Y, masks)\n",
        "        correct_pred = tf.equal(self.prediction, mask_label)\n",
        "        correct_index = tf.cast(correct_pred, tf.float32)\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgoSE1lyhXxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size_layer = 128\n",
        "num_layers = 2\n",
        "embedded_size = 128\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epoch = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFE4cILphZkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "653af29a-d70b-496e-dcb5-357235a88365"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "model = Translator(size_layer, num_layers, embedded_size, len(malay_dictionary), \n",
        "                len(english_dictionary), learning_rate,batch_size)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "reduction_indices is deprecated, use axis instead\n",
            "WARNING:tensorflow:From <ipython-input-18-ed4a1adc6799>:8: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-18-ed4a1adc6799>:29: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-18-ed4a1adc6799>:32: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnULuUDyhbm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str_idx(corpus, dic):\n",
        "    X = []\n",
        "    for i in corpus:\n",
        "        ints = []\n",
        "        for k in i.split():\n",
        "            ints.append(dic.get(k,UNK))\n",
        "        X.append(ints)\n",
        "    return X\n",
        "\n",
        "def pad_sentence_batch(sentence_batch, pad_int):\n",
        "    padded_seqs = []\n",
        "    seq_lens = []\n",
        "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
        "    for sentence in sentence_batch:\n",
        "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
        "        seq_lens.append(len(sentence))\n",
        "    return padded_seqs, seq_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqXsFhLOhd9a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "23a0a655-5121-41b6-d4bd-0e9413e1985d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(malay, english)\n",
        "\n",
        "train_X[0], train_Y[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tetapi selepas itu , perkara - perkara yang cukup mantap dan seragam cantik .',\n",
              " 'But after that , things were pretty steady and pretty uniform . EOS')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ-TNEt7hf45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = str_idx(train_X, malay_dictionary)\n",
        "test_X = str_idx(test_X, malay_dictionary)\n",
        "train_Y = str_idx(train_Y, english_dictionary)\n",
        "test_Y = str_idx(test_Y, english_dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVuUEem5hh7S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "8fa84a13-4848-4ef3-964b-a7e19a03e503"
      },
      "source": [
        "import tqdm\n",
        "import numpy as np\n",
        "\n",
        "for e in range(epoch):\n",
        "    pbar = tqdm.tqdm(\n",
        "        range(0, len(train_X), batch_size), desc = 'minibatch loop')\n",
        "    train_loss, train_acc, test_loss, test_acc = [], [], [], []\n",
        "    for i in pbar:\n",
        "        index = min(i + batch_size, len(train_X))\n",
        "        maxlen = max([len(s) for s in train_X[i : index] + train_Y[i : index]])\n",
        "        batch_x, seq_x = pad_sentence_batch(train_X[i : index], PAD)\n",
        "        batch_y, seq_y = pad_sentence_batch(train_Y[i : index], PAD)\n",
        "        feed = {model.X: batch_x,\n",
        "                model.Y: batch_y}\n",
        "        accuracy, loss, _ = sess.run([model.accuracy,model.cost,model.optimizer],\n",
        "                                    feed_dict = feed)\n",
        "        train_loss.append(loss)\n",
        "        train_acc.append(accuracy)\n",
        "        pbar.set_postfix(cost = loss, accuracy = accuracy)\n",
        "        \n",
        "    pbar = tqdm.tqdm(\n",
        "        range(0, len(test_X), batch_size), desc = 'minibatch loop')\n",
        "    for i in pbar:\n",
        "        index = min(i + batch_size, len(test_X))\n",
        "        batch_x, seq_x = pad_sentence_batch(test_X[i : index], PAD)\n",
        "        batch_y, seq_y = pad_sentence_batch(test_Y[i : index], PAD)\n",
        "        feed = {model.X: batch_x,\n",
        "                model.Y: batch_y,}\n",
        "        accuracy, loss = sess.run([model.accuracy,model.cost],\n",
        "                                    feed_dict = feed)\n",
        "\n",
        "        test_loss.append(loss)\n",
        "        test_acc.append(accuracy)\n",
        "        pbar.set_postfix(cost = loss, accuracy = accuracy)\n",
        "    \n",
        "    print('epoch %d, training avg loss %f, training avg acc %f'%(e+1,\n",
        "                                                                 np.mean(train_loss),np.mean(train_acc)))\n",
        "    print('epoch %d, testing avg loss %f, testing avg acc %f'%(e+1,\n",
        "                                                              np.mean(test_loss),np.mean(test_acc)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 469/469 [03:55<00:00,  2.61it/s, accuracy=0.135, cost=5.51]\n",
            "minibatch loop: 100%|██████████| 157/157 [00:42<00:00,  3.77it/s, accuracy=0.25, cost=5.17]\n",
            "minibatch loop:   0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, training avg loss 5.995502, training avg acc 0.136907\n",
            "epoch 1, testing avg loss 5.442404, testing avg acc 0.194961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 469/469 [03:58<00:00,  2.56it/s, accuracy=0.206, cost=4.88]\n",
            "minibatch loop: 100%|██████████| 157/157 [00:42<00:00,  4.08it/s, accuracy=0.317, cost=4.76]\n",
            "minibatch loop:   0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2, training avg loss 5.053892, training avg acc 0.233081\n",
            "epoch 2, testing avg loss 4.989242, testing avg acc 0.272881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 469/469 [03:59<00:00,  2.61it/s, accuracy=0.271, cost=3.92]\n",
            "minibatch loop: 100%|██████████| 157/157 [00:42<00:00,  4.11it/s, accuracy=0.417, cost=4.11]\n",
            "minibatch loop:   0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3, training avg loss 4.451171, training avg acc 0.319249\n",
            "epoch 3, testing avg loss 4.443514, testing avg acc 0.364786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 469/469 [03:57<00:00,  2.57it/s, accuracy=0.406, cost=3.05]\n",
            "minibatch loop: 100%|██████████| 157/157 [00:42<00:00,  4.19it/s, accuracy=0.433, cost=3.73]\n",
            "minibatch loop:   0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4, training avg loss 3.799646, training avg acc 0.410866\n",
            "epoch 4, testing avg loss 3.987099, testing avg acc 0.440425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 469/469 [03:57<00:00,  2.56it/s, accuracy=0.535, cost=2.41]\n",
            "minibatch loop: 100%|██████████| 157/157 [00:42<00:00,  4.13it/s, accuracy=0.467, cost=3.24]\n",
            "minibatch loop:   0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5, training avg loss 3.284848, training avg acc 0.473120\n",
            "epoch 5, testing avg loss 3.759744, testing avg acc 0.475238\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 469/469 [03:58<00:00,  2.57it/s, accuracy=0.581, cost=1.92]\n",
            "minibatch loop: 100%|██████████| 157/157 [00:42<00:00,  4.00it/s, accuracy=0.517, cost=2.9]\n",
            "minibatch loop:   0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6, training avg loss 2.900651, training avg acc 0.512246\n",
            "epoch 6, testing avg loss 3.606861, testing avg acc 0.499303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 469/469 [03:57<00:00,  2.56it/s, accuracy=0.69, cost=1.57]\n",
            "minibatch loop: 100%|██████████| 157/157 [00:41<00:00,  4.07it/s, accuracy=0.517, cost=2.91]\n",
            "minibatch loop:   0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 7, training avg loss 2.589170, training avg acc 0.542997\n",
            "epoch 7, testing avg loss 3.579119, testing avg acc 0.510328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 469/469 [03:55<00:00,  2.62it/s, accuracy=0.781, cost=1.25]\n",
            "minibatch loop: 100%|██████████| 157/157 [00:41<00:00,  4.11it/s, accuracy=0.533, cost=3.02]\n",
            "minibatch loop:   0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 8, training avg loss 2.306392, training avg acc 0.574516\n",
            "epoch 8, testing avg loss 3.567849, testing avg acc 0.511956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 469/469 [03:56<00:00,  2.61it/s, accuracy=0.755, cost=1.25]\n",
            "minibatch loop: 100%|██████████| 157/157 [00:42<00:00,  4.11it/s, accuracy=0.6, cost=2.62]\n",
            "minibatch loop:   0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 9, training avg loss 2.071074, training avg acc 0.605599\n",
            "epoch 9, testing avg loss 3.636268, testing avg acc 0.524038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 469/469 [03:56<00:00,  2.58it/s, accuracy=0.806, cost=0.876]\n",
            "minibatch loop: 100%|██████████| 157/157 [00:41<00:00,  4.15it/s, accuracy=0.583, cost=2.7]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 10, training avg loss 1.869579, training avg acc 0.635038\n",
            "epoch 10, testing avg loss 3.508983, testing avg acc 0.542806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYrZFlGaij3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}