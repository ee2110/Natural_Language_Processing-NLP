{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Analytics_Service.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e89drEGGycbZ",
        "outputId": "b3cf3909-e844-4f41-fb48-23a8f13fa94e"
      },
      "source": [
        "pip install azure-ai-textanalytics==5.1.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting azure-ai-textanalytics==5.1.0\n",
            "  Downloading azure_ai_textanalytics-5.1.0-py2.py3-none-any.whl (153 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 51 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 61 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 81 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 92 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 102 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 112 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 122 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 133 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 143 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153 kB 15.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from azure-ai-textanalytics==5.1.0) (1.15.0)\n",
            "Collecting azure-core<2.0.0,>=1.14.0\n",
            "  Downloading azure_core-1.17.0-py2.py3-none-any.whl (165 kB)\n",
            "\u001b[K     |████████████████████████████████| 165 kB 37.4 MB/s \n",
            "\u001b[?25hCollecting msrest>=0.6.21\n",
            "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting azure-common~=1.1\n",
            "  Downloading azure_common-1.1.27-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from azure-core<2.0.0,>=1.14.0->azure-ai-textanalytics==5.1.0) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-textanalytics==5.1.0) (1.3.0)\n",
            "Collecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-textanalytics==5.1.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-ai-textanalytics==5.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-ai-textanalytics==5.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-ai-textanalytics==5.1.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-textanalytics==5.1.0) (3.1.1)\n",
            "Installing collected packages: isodate, msrest, azure-core, azure-common, azure-ai-textanalytics\n",
            "Successfully installed azure-ai-textanalytics-5.1.0 azure-common-1.1.27 azure-core-1.17.0 isodate-0.6.0 msrest-0.6.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9a-6X4oyp-R"
      },
      "source": [
        "file1 = open('azure.txt', 'r')\n",
        "lines = file1.readlines()\n",
        "key = lines[0].rstrip(\"\\n\")\n",
        "endpoint = lines[1]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeGjPR_T5f68"
      },
      "source": [
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "def authenticate_client():\n",
        "    ta_credential = AzureKeyCredential(key)\n",
        "    text_analytics_client = TextAnalyticsClient(\n",
        "            endpoint=endpoint, \n",
        "            credential=ta_credential)\n",
        "    return text_analytics_client\n",
        "\n",
        "client = authenticate_client()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcxL7v2IK2q4"
      },
      "source": [
        "# **Sentiment analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11r1F_hz5l_W",
        "outputId": "d250894f-a65b-4846-9ea3-0bd0c24c65ec"
      },
      "source": [
        "def sentiment_analysis_example(client):\n",
        "\n",
        "    documents = [\"I had the best day of my life. I wish you were there with me.\"]\n",
        "    response = client.analyze_sentiment(documents=documents)[0]\n",
        "    print(\"Document Sentiment: {}\".format(response.sentiment))\n",
        "    print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
        "        response.confidence_scores.positive,\n",
        "        response.confidence_scores.neutral,\n",
        "        response.confidence_scores.negative,\n",
        "    ))\n",
        "    for idx, sentence in enumerate(response.sentences):\n",
        "        print(\"Sentence: {}\".format(sentence.text))\n",
        "        print(\"Sentence {} sentiment: {}\".format(idx+1, sentence.sentiment))\n",
        "        print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
        "            sentence.confidence_scores.positive,\n",
        "            sentence.confidence_scores.neutral,\n",
        "            sentence.confidence_scores.negative,\n",
        "        ))\n",
        "          \n",
        "sentiment_analysis_example(client)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Sentiment: positive\n",
            "Overall scores: positive=1.00; neutral=0.00; negative=0.00 \n",
            "\n",
            "Sentence: I had the best day of my life.\n",
            "Sentence 1 sentiment: positive\n",
            "Sentence score:\n",
            "Positive=1.00\n",
            "Neutral=0.00\n",
            "Negative=0.00\n",
            "\n",
            "Sentence: I wish you were there with me.\n",
            "Sentence 2 sentiment: neutral\n",
            "Sentence score:\n",
            "Positive=0.21\n",
            "Neutral=0.77\n",
            "Negative=0.02\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "porILAWl6cMl"
      },
      "source": [
        "**Test on other dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLw50OfT698t"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCoYqAVE5qPy",
        "outputId": "9a9f8881-3257-40eb-faa4-82f755f811ac"
      },
      "source": [
        "data = pd.read_csv('combined_data.csv')\n",
        "sentences = data['text'].tolist()\n",
        "labels = data['sentiment'].tolist()\n",
        "print('Total sentences ', len(sentences))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sentences  1992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3pdUOcr7Igj",
        "outputId": "79816c76-ffb1-46f4-f22c-745bc9820ed6"
      },
      "source": [
        "sentiment_count = {}\n",
        "\n",
        "for element in labels:\n",
        "   if element in sentiment_count:\n",
        "      sentiment_count[element] += 1\n",
        "   else:\n",
        "      sentiment_count[element] = 1\n",
        "\n",
        "for key, value in sentiment_count.items():\n",
        "   print(f\"{key}: {value}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 996\n",
            "1: 996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XvV_PWc9Eme"
      },
      "source": [
        "**0** indicated **negative sentiment**\n",
        "\n",
        "**1** indicated **positive sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YWNDM9y8GKF",
        "outputId": "41cee37d-aa0a-42c4-85f1-83f80ff1f141"
      },
      "source": [
        "from random import seed\n",
        "from random import randint\n",
        "seed(1)\n",
        "\n",
        "n = randint(0, len(sentences))\n",
        "\n",
        "data1 = sentences[n]\n",
        "true_label1 = labels[n]\n",
        "print(data1)\n",
        "print(true_label1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reversible plug works great.\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW8iTln_-FJi",
        "outputId": "d3a83739-d3d7-4ae6-f4c8-63e9b5b9c9cc"
      },
      "source": [
        "response = client.analyze_sentiment(documents=[data1])[0]\n",
        "print(\"Predicted Sentiment: {}\".format(response.sentiment))\n",
        "print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
        "    response.confidence_scores.positive,\n",
        "    response.confidence_scores.neutral,\n",
        "    response.confidence_scores.negative,\n",
        "))\n",
        "print(\"Sentiment Prediction Score :\" ,round(response.confidence_scores.positive))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Sentiment: positive\n",
            "Overall scores: positive=1.00; neutral=0.00; negative=0.00 \n",
            "\n",
            "Sentiment Prediction Score : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSPLpmqS_jut"
      },
      "source": [
        "predicted_result = []\n",
        "for i in range(len(sentences)):\n",
        "  test_data = [sentences[i]]\n",
        "  response = client.analyze_sentiment(documents=test_data)[0]\n",
        "  result = round(response.confidence_scores.positive)\n",
        "  predicted_result.append(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3_BlCcVIY-A"
      },
      "source": [
        "Due to using free tier, the calls are limited, therefore, only small portion of test data predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mgssek0IIQI",
        "outputId": "2f30519a-cedf-48e4-df21-43cba395d214"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_true = labels[:len(predicted_result)]\n",
        "y_pred = predicted_result\n",
        "print('The overall prediction score is', accuracy_score(y_true, y_pred))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The overall prediction score is 0.9035532994923858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZzPYGOqKd65"
      },
      "source": [
        "# **Opinion mining**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbISov_0IBxn",
        "outputId": "1e83673d-5a36-42df-8a77-11fce68b7bf8"
      },
      "source": [
        "def sentiment_analysis_with_opinion_mining_example(client):\n",
        "\n",
        "    documents = [\n",
        "        \"The food and service were unacceptable, but the concierge were nice\"\n",
        "    ]\n",
        "\n",
        "    result = client.analyze_sentiment(documents, show_opinion_mining=True)\n",
        "    doc_result = [doc for doc in result if not doc.is_error]\n",
        "\n",
        "    positive_reviews = [doc for doc in doc_result if doc.sentiment == \"positive\"]\n",
        "    negative_reviews = [doc for doc in doc_result if doc.sentiment == \"negative\"]\n",
        "\n",
        "    positive_mined_opinions = []\n",
        "    mixed_mined_opinions = []\n",
        "    negative_mined_opinions = []\n",
        "\n",
        "    for document in doc_result:\n",
        "        print(\"Document Sentiment: {}\".format(document.sentiment))\n",
        "        print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
        "            document.confidence_scores.positive,\n",
        "            document.confidence_scores.neutral,\n",
        "            document.confidence_scores.negative,\n",
        "        ))\n",
        "        for sentence in document.sentences:\n",
        "            print(\"Sentence: {}\".format(sentence.text))\n",
        "            print(\"Sentence sentiment: {}\".format(sentence.sentiment))\n",
        "            print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
        "                sentence.confidence_scores.positive,\n",
        "                sentence.confidence_scores.neutral,\n",
        "                sentence.confidence_scores.negative,\n",
        "            ))\n",
        "            for mined_opinion in sentence.mined_opinions:\n",
        "                target = mined_opinion.target\n",
        "                print(\"......'{}' target '{}'\".format(target.sentiment, target.text))\n",
        "                print(\"......Target score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
        "                    target.confidence_scores.positive,\n",
        "                    target.confidence_scores.negative,\n",
        "                ))\n",
        "                for assessment in mined_opinion.assessments:\n",
        "                    print(\"......'{}' assessment '{}'\".format(assessment.sentiment, assessment.text))\n",
        "                    print(\"......Assessment score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
        "                        assessment.confidence_scores.positive,\n",
        "                        assessment.confidence_scores.negative,\n",
        "                    ))\n",
        "            print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "          \n",
        "sentiment_analysis_with_opinion_mining_example(client)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Sentiment: positive\n",
            "Overall scores: positive=0.84; neutral=0.00; negative=0.16 \n",
            "\n",
            "Sentence: The food and service were unacceptable, but the concierge were nice\n",
            "Sentence sentiment: positive\n",
            "Sentence score:\n",
            "Positive=0.84\n",
            "Neutral=0.00\n",
            "Negative=0.16\n",
            "\n",
            "......'negative' target 'food'\n",
            "......Target score:\n",
            "......Positive=0.01\n",
            "......Negative=0.99\n",
            "\n",
            "......'negative' assessment 'unacceptable'\n",
            "......Assessment score:\n",
            "......Positive=0.01\n",
            "......Negative=0.99\n",
            "\n",
            "......'negative' target 'service'\n",
            "......Target score:\n",
            "......Positive=0.01\n",
            "......Negative=0.99\n",
            "\n",
            "......'negative' assessment 'unacceptable'\n",
            "......Assessment score:\n",
            "......Positive=0.01\n",
            "......Negative=0.99\n",
            "\n",
            "......'positive' target 'concierge'\n",
            "......Target score:\n",
            "......Positive=1.00\n",
            "......Negative=0.00\n",
            "\n",
            "......'positive' assessment 'nice'\n",
            "......Assessment score:\n",
            "......Positive=1.00\n",
            "......Negative=0.00\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4-lelDJLl0e"
      },
      "source": [
        "# **Language detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFYVG69WLXtg",
        "outputId": "34bb8097-ed93-4aae-d7b7-fe92eb34abb4"
      },
      "source": [
        "def language_detection_example(client):\n",
        "    try:\n",
        "        documents = [\"我在用电脑\"]\n",
        "        response = client.detect_language(documents = documents, country_hint = 'us')[0]\n",
        "        print(\"Language: \", response.primary_language.name)\n",
        "\n",
        "    except Exception as err:\n",
        "        print(\"Encountered exception. {}\".format(err))\n",
        "language_detection_example(client)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Language:  Chinese_Simplified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgW5w40MMKmw"
      },
      "source": [
        "# **Named Entity Recognition (NER)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE0K_XEgMDc2",
        "outputId": "4722795c-0340-4a8f-c655-4d3fc22d26d2"
      },
      "source": [
        "def entity_recognition_example(client):\n",
        "\n",
        "    try:\n",
        "        documents = [\"I had a wonderful trip to Seattle last week.\"]\n",
        "        result = client.recognize_entities(documents = documents)[0]\n",
        "\n",
        "        print(\"Named Entities:\\n\")\n",
        "        for entity in result.entities:\n",
        "            print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
        "                    \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\tLength: \\t\", entity.length, \"\\tOffset: \\t\", entity.offset, \"\\n\")\n",
        "\n",
        "    except Exception as err:\n",
        "        print(\"Encountered exception. {}\".format(err))\n",
        "entity_recognition_example(client)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Named Entities:\n",
            "\n",
            "\tText: \t trip \tCategory: \t Event \tSubCategory: \t None \n",
            "\tConfidence Score: \t 0.73 \tLength: \t 4 \tOffset: \t 18 \n",
            "\n",
            "\tText: \t Seattle \tCategory: \t Location \tSubCategory: \t GPE \n",
            "\tConfidence Score: \t 1.0 \tLength: \t 7 \tOffset: \t 26 \n",
            "\n",
            "\tText: \t last week \tCategory: \t DateTime \tSubCategory: \t DateRange \n",
            "\tConfidence Score: \t 0.8 \tLength: \t 9 \tOffset: \t 34 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgHM9qngNQWO"
      },
      "source": [
        "# **Personally Identifiable Information (PII) recognition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIlpVPsMMi0m",
        "outputId": "a55bfb01-4fa8-45ac-f1d0-6b6c6c5f37bf"
      },
      "source": [
        "def pii_recognition_example(client):\n",
        "    documents = [\n",
        "        \"The office contact number is 859-98-0987.\",\n",
        "        \"While the director personal contact phone number is 555-555-5555.\"\n",
        "    ]\n",
        "    response = client.recognize_pii_entities(documents, language=\"en\")\n",
        "    result = [doc for doc in response if not doc.is_error]\n",
        "    for doc in result:\n",
        "        print(\"Redacted Text: {}\".format(doc.redacted_text))\n",
        "        for entity in doc.entities:\n",
        "            print(\"Entity: {}\".format(entity.text))\n",
        "            print(\"\\tCategory: {}\".format(entity.category))\n",
        "            print(\"\\tConfidence Score: {}\".format(entity.confidence_score))\n",
        "            print(\"\\tOffset: {}\".format(entity.offset))\n",
        "            print(\"\\tLength: {}\".format(entity.length))\n",
        "pii_recognition_example(client)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Redacted Text: The office contact number is 859-98-0987.\n",
            "Redacted Text: While the ******** personal contact phone number is ************.\n",
            "Entity: director\n",
            "\tCategory: PersonType\n",
            "\tConfidence Score: 0.95\n",
            "\tOffset: 10\n",
            "\tLength: 8\n",
            "Entity: 555-555-5555\n",
            "\tCategory: PhoneNumber\n",
            "\tConfidence Score: 0.8\n",
            "\tOffset: 52\n",
            "\tLength: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Is-SUoUNsuD"
      },
      "source": [
        "seems like some context is not captured as entity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoHtMzD6N2cu"
      },
      "source": [
        "# **Entity linking**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwHzliZJNTpE",
        "outputId": "ed73c6c0-5fbb-4960-a2dd-43bbc966bea7"
      },
      "source": [
        "def entity_linking_example(client):\n",
        "\n",
        "    try:\n",
        "        documents = [\"\"\"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, \n",
        "        to develop and sell BASIC interpreters for the Altair 8800. \n",
        "        During his career at Microsoft, Gates held the positions of chairman,\n",
        "        chief executive officer, president and chief software architect, \n",
        "        while also being the largest individual shareholder until May 2014.\"\"\"]\n",
        "        result = client.recognize_linked_entities(documents = documents)[0]\n",
        "\n",
        "        print(\"Linked Entities:\\n\")\n",
        "        for entity in result.entities:\n",
        "            print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url,\n",
        "            \"\\n\\tData Source: \", entity.data_source)\n",
        "            print(\"\\tMatches:\")\n",
        "            for match in entity.matches:\n",
        "                print(\"\\t\\tText:\", match.text)\n",
        "                print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
        "                print(\"\\t\\tOffset: {}\".format(match.offset))\n",
        "                print(\"\\t\\tLength: {}\".format(match.length))\n",
        "            \n",
        "    except Exception as err:\n",
        "        print(\"Encountered exception. {}\".format(err))\n",
        "entity_linking_example(client)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linked Entities:\n",
            "\n",
            "\tName:  Microsoft \tId:  Microsoft \tUrl:  https://en.wikipedia.org/wiki/Microsoft \n",
            "\tData Source:  Wikipedia\n",
            "\tMatches:\n",
            "\t\tText: Microsoft\n",
            "\t\tConfidence Score: 0.55\n",
            "\t\tOffset: 0\n",
            "\t\tLength: 9\n",
            "\t\tText: Microsoft\n",
            "\t\tConfidence Score: 0.55\n",
            "\t\tOffset: 168\n",
            "\t\tLength: 9\n",
            "\tName:  Bill Gates \tId:  Bill Gates \tUrl:  https://en.wikipedia.org/wiki/Bill_Gates \n",
            "\tData Source:  Wikipedia\n",
            "\tMatches:\n",
            "\t\tText: Bill Gates\n",
            "\t\tConfidence Score: 0.63\n",
            "\t\tOffset: 25\n",
            "\t\tLength: 10\n",
            "\t\tText: Gates\n",
            "\t\tConfidence Score: 0.63\n",
            "\t\tOffset: 179\n",
            "\t\tLength: 5\n",
            "\tName:  Paul Allen \tId:  Paul Allen \tUrl:  https://en.wikipedia.org/wiki/Paul_Allen \n",
            "\tData Source:  Wikipedia\n",
            "\tMatches:\n",
            "\t\tText: Paul Allen\n",
            "\t\tConfidence Score: 0.60\n",
            "\t\tOffset: 40\n",
            "\t\tLength: 10\n",
            "\tName:  April 4 \tId:  April 4 \tUrl:  https://en.wikipedia.org/wiki/April_4 \n",
            "\tData Source:  Wikipedia\n",
            "\tMatches:\n",
            "\t\tText: April 4\n",
            "\t\tConfidence Score: 0.32\n",
            "\t\tOffset: 54\n",
            "\t\tLength: 7\n",
            "\tName:  BASIC \tId:  BASIC \tUrl:  https://en.wikipedia.org/wiki/BASIC \n",
            "\tData Source:  Wikipedia\n",
            "\tMatches:\n",
            "\t\tText: BASIC\n",
            "\t\tConfidence Score: 0.33\n",
            "\t\tOffset: 98\n",
            "\t\tLength: 5\n",
            "\tName:  Altair 8800 \tId:  Altair 8800 \tUrl:  https://en.wikipedia.org/wiki/Altair_8800 \n",
            "\tData Source:  Wikipedia\n",
            "\tMatches:\n",
            "\t\tText: Altair 8800\n",
            "\t\tConfidence Score: 0.88\n",
            "\t\tOffset: 125\n",
            "\t\tLength: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v943bmvkOMDD"
      },
      "source": [
        "# **Key phrase extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbaxEC-VOKrN",
        "outputId": "efc8218e-f662-497f-abb8-ad77dae96aec"
      },
      "source": [
        "def key_phrase_extraction_example(client):\n",
        "\n",
        "    try:\n",
        "        documents = [\"I going to travel around the World for 5 years\"]\n",
        "\n",
        "        response = client.extract_key_phrases(documents = documents)[0]\n",
        "\n",
        "        if not response.is_error:\n",
        "            print(\"\\tKey Phrases:\")\n",
        "            for phrase in response.key_phrases:\n",
        "                print(\"\\t\\t\", phrase)\n",
        "        else:\n",
        "            print(response.id, response.error)\n",
        "\n",
        "    except Exception as err:\n",
        "        print(\"Encountered exception. {}\".format(err))\n",
        "        \n",
        "key_phrase_extraction_example(client)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tKey Phrases:\n",
            "\t\t World\n",
            "\t\t 5 years\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1dwULmIPY_2"
      },
      "source": [
        "# **Extract health entities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaCN3OXzOlSZ"
      },
      "source": [
        "def health_example(client):\n",
        "    documents = [\n",
        "        \"\"\"\n",
        "        Patient needs to take 50 mg of ibuprofen.\n",
        "        \"\"\"\n",
        "    ]\n",
        "\n",
        "    poller = client.begin_analyze_healthcare_entities(documents)\n",
        "    result = poller.result()\n",
        "\n",
        "    docs = [doc for doc in result if not doc.is_error]\n",
        "\n",
        "    for idx, doc in enumerate(docs):\n",
        "        for entity in doc.entities:\n",
        "            print(\"Entity: {}\".format(entity.text))\n",
        "            print(\"...Normalized Text: {}\".format(entity.normalized_text))\n",
        "            print(\"...Category: {}\".format(entity.category))\n",
        "            print(\"...Subcategory: {}\".format(entity.subcategory))\n",
        "            print(\"...Offset: {}\".format(entity.offset))\n",
        "            print(\"...Confidence score: {}\".format(entity.confidence_score))\n",
        "        for relation in doc.entity_relations:\n",
        "            print(\"Relation of type: {} has the following roles\".format(relation.relation_type))\n",
        "            for role in relation.roles:\n",
        "                print(\"...Role '{}' with entity '{}'\".format(role.name, role.entity.text))\n",
        "        print(\"------------------------------------------\")\n",
        "health_example(client)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ublAV8ERQD6B"
      },
      "source": [
        "**Healthcare analysis is currently only supported for Text Analytics Standard tier.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqmnN_4CP6l5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}