{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS_Recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_brnLNm9n0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "276f49c1-34a1-4892-a691-8407c782a0a1"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/part-of-speech/pos-data-v3.json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-11 07:47:07--  https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/part-of-speech/pos-data-v3.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3085086 (2.9M) [text/plain]\n",
            "Saving to: ‘pos-data-v3.json’\n",
            "\n",
            "\rpos-data-v3.json      0%[                    ]       0  --.-KB/s               \rpos-data-v3.json    100%[===================>]   2.94M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-09-11 07:47:08 (63.2 MB/s) - ‘pos-data-v3.json’ saved [3085086/3085086]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzCfd7B1Rb9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40840f07-cd33-4709-9217-b2247682901b"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('pos-data-v3.json') as fopen:\n",
        "  pos = json.load(fopen)\n",
        "  \n",
        "pos[0], len(pos)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Sampul', 'sampul', 'NOUN'], 103695)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKRWQAdpRePD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = pos[:10000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wsqYlMQRgMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b898e42f-f611-41c0-9fc3-3f5d9c1d8bc7"
      },
      "source": [
        "pos[:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Sampul', 'sampul', 'NOUN'],\n",
              " ['dari', 'dari', 'ADP'],\n",
              " ['dua', 'dua', 'NUM'],\n",
              " ['singel', 'singel', 'NOUN'],\n",
              " ['pertama', 'pertama', 'NUM'],\n",
              " ['difoto', 'difoto', 'VERB'],\n",
              " ['oleh', 'oleh', 'ADP'],\n",
              " ['Emma', 'emma', 'PROPN'],\n",
              " ['Summerton', 'summerton', 'PROPN'],\n",
              " ['pada', 'pada', 'ADP']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p3ocbw8RhtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def process_string(string):\n",
        "    string = re.sub('[^A-Za-z0-9\\-\\/ ]+', ' ', string).split()\n",
        "    return [to_title(y.strip()) for y in string]\n",
        "  \n",
        "def to_title(string):\n",
        "    if string.isupper():\n",
        "        string = string.title()\n",
        "    return string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEl_wAj6Rj8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afb2a996-13a6-42ca-f502-4606bf2de22e"
      },
      "source": [
        "process_string('H')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['H']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K1hcPQaRl17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "38e90987-ef5a-4247-a524-2ee8d9a38d0b"
      },
      "source": [
        "texts, labels = [], []\n",
        "for i in pos:\n",
        "    try:\n",
        "        texts.append(process_string(i[0])[0])\n",
        "        labels.append(i[-1])\n",
        "    except Exception as e:\n",
        "        print(e, i)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "list index out of range ['%', '%', 'SYM']\n",
            "list index out of range ['%', '%', 'SYM']\n",
            "list index out of range ['*', '*', 'SYM']\n",
            "list index out of range ['뭘봐', '뭘봐', 'PROPN']\n",
            "list index out of range ['%', '%', 'SYM']\n",
            "list index out of range ['ひ', 'ひ', 'PROPN']\n",
            "list index out of range ['ヒ', 'ヒ', 'PROPN']\n",
            "list index out of range ['形聲', '形聲', 'NOUN']\n",
            "list index out of range ['°', '°', 'SYM']\n",
            "list index out of range ['汉', '汉', 'PROPN']\n",
            "list index out of range ['东', '东', 'PROPN']\n",
            "list index out of range ['王', '王', 'PROPN']\n",
            "list index out of range ['（', '（', 'PROPN']\n",
            "list index out of range ['伊', '伊', 'PROPN']\n",
            "list index out of range ['）', '）', 'PROPN']\n",
            "list index out of range ['ȝ', 'ȝ', 'PROPN']\n",
            "list index out of range ['%', '%', 'SYM']\n",
            "list index out of range ['°', '°', 'SYM']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA3HEkiCRn4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "35316e6f-e20c-4703-cd13-a498de5e0136"
      },
      "source": [
        "texts[-10:], labels[-10:]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['mahasiswa',\n",
              "  'Apakah',\n",
              "  'ada',\n",
              "  'perkembangan',\n",
              "  'perbaikan',\n",
              "  'Platz',\n",
              "  'kemudian',\n",
              "  'kembali',\n",
              "  'kepada',\n",
              "  'rancangan'],\n",
              " ['NOUN',\n",
              "  'ADV',\n",
              "  'VERB',\n",
              "  'NOUN',\n",
              "  'NOUN',\n",
              "  'PROPN',\n",
              "  'ADV',\n",
              "  'VERB',\n",
              "  'ADP',\n",
              "  'NOUN'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1_d-rjQRrbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "word2idx = {'PAD': 0,'UNK':1}\n",
        "tag2idx = {'PAD': 0}\n",
        "char2idx = {'PAD': 0}\n",
        "word_idx = 2\n",
        "tag_idx = 1\n",
        "char_idx = 1\n",
        "\n",
        "def parse_XY(texts, labels):\n",
        "    global word2idx, tag2idx, char2idx, word_idx, tag_idx, char_idx\n",
        "    X, Y, sentences = [], [], []\n",
        "    for no, text in enumerate(texts):\n",
        "        tag = labels[no]\n",
        "        for c in text:\n",
        "            if c not in char2idx:\n",
        "                char2idx[c] = char_idx\n",
        "                char_idx += 1\n",
        "        if tag not in tag2idx:\n",
        "            tag2idx[tag] = tag_idx\n",
        "            tag_idx += 1\n",
        "        Y.append(tag2idx[tag])\n",
        "        if text not in word2idx:\n",
        "            word2idx[text] = word_idx\n",
        "            word_idx += 1\n",
        "        X.append(word2idx[text])\n",
        "        sentences.append(text)\n",
        "    return X, np.array(Y), sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCl_H4zVRt9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, Y, sentences = parse_XY(texts, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTjMSXjlRvxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len = 50\n",
        "def iter_seq(x):\n",
        "    return np.array([x[i: i+seq_len] for i in range(0, len(x)-seq_len, 1)])\n",
        "\n",
        "def to_train_seq(*args):\n",
        "    return [iter_seq(x) for x in args]\n",
        "\n",
        "def generate_char_seq(batch):\n",
        "    x = [[len(i) for i in k] for k in batch]\n",
        "    len_maxlen = max([len(i) for i in x])\n",
        "    maxlen = max([j for i in x for j in i])\n",
        "    temp = np.zeros((len(batch), len_maxlen,maxlen),dtype=np.int32)\n",
        "    for i in range(len(batch)):\n",
        "        for k in range(len_maxlen):\n",
        "            if k < len(batch[i]):\n",
        "                for no, c in enumerate(batch[i][k][::-1]):\n",
        "                    temp[i,k,-1-no] = char2idx[c]\n",
        "    return temp\n",
        "  \n",
        "def word_seq(batch):\n",
        "  temp = np.zeros((batch.shape[0], batch.shape[1]),dtype=np.int32)\n",
        "  for i in range(batch.shape[0]):\n",
        "    row = batch[i][::-1]\n",
        "    for no in range(batch.shape[1]):\n",
        "      temp[i,-1-no] = word2idx.get(row[no], 1)\n",
        "  return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN48PzBaRzVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx2word={idx: tag for tag, idx in word2idx.items()}\n",
        "idx2tag = {i: w for w, i in tag2idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi1LiWKTR1O5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "901fa70b-88c3-41ed-b4b6-916097a5cc57"
      },
      "source": [
        "X_seq, Y_seq = to_train_seq(sentences, Y)\n",
        "X_char_seq = generate_char_seq(X_seq)\n",
        "X_seq.shape, X_char_seq.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9932, 50), (9932, 50, 23))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4nbEXh6R22-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "187a2004-00c4-4fc1-9cbe-3692add7defe"
      },
      "source": [
        "text_sample = ['Husein makan ayam', 'Husein suka ayam dan itik']\n",
        "text_sample = [s.split() for s in text_sample]\n",
        "text_sample"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Husein', 'makan', 'ayam'], ['Husein', 'suka', 'ayam', 'dan', 'itik']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuWf9puFR4qA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbc4b28f-b6ea-42fe-dd6d-69958ce3db6c"
      },
      "source": [
        "generate_char_seq(text_sample).shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 5, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxpDf0u6R6P6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = word_seq(X_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIDxMw-TR8OK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "de83b542-204c-4880-eaaf-823e80fcde2e"
      },
      "source": [
        "Y_seq[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, 1, 3, 4, 2, 5, 5, 2, 1, 5, 3, 6, 3, 1, 7, 4, 2, 7, 8, 4,\n",
              "       2, 4, 1, 2, 1, 5, 5, 7, 1, 4, 5, 9, 4, 7, 2, 1, 1, 1, 4, 2, 1, 7,\n",
              "       2, 5, 5, 6, 4, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "San6-7hGR-B_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b0b6453-2285-4fc8-ecf7-a200e9dec055"
      },
      "source": [
        "idx2tag[1]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NOUN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUW83us8R_1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d973888-2772-42c7-9057-8dca8b535da4"
      },
      "source": [
        "idx2word[2]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sampul'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCtOzJbdSBPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b7cdbf71-9027-463d-8f27-08e9be87cad1"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "       19,  8, 20, 21, 22, 23, 24, 25, 26, 12, 27,  2, 28, 25, 29, 30, 31,\n",
              "       32, 33, 26, 34, 35, 36, 37, 26, 38, 39,  8, 40, 41, 15, 22, 11],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-nbMhaaSDAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, test_X, char_train_X, char_test_X, train_Y, test_Y = train_test_split(X, \n",
        "                                                                               X_char_seq, \n",
        "                                                                               Y_seq, \n",
        "                                                                               test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-Z2zD5HSFpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_word,\n",
        "        dim_char,\n",
        "        dropout,\n",
        "        learning_rate,\n",
        "        hidden_size_char,\n",
        "        hidden_size_word,\n",
        "        num_layers,\n",
        "    ):\n",
        "        def cells(size, reuse = False):\n",
        "            return tf.contrib.rnn.DropoutWrapper(\n",
        "                tf.nn.rnn_cell.LSTMCell(\n",
        "                    size,\n",
        "                    initializer = tf.orthogonal_initializer(),\n",
        "                    reuse = reuse,\n",
        "                ),\n",
        "                state_keep_prob = dropout,\n",
        "                output_keep_prob = dropout,\n",
        "            )\n",
        "          \n",
        "        self.word_ids = tf.placeholder(tf.int32, shape = [None, None])\n",
        "        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None])\n",
        "        self.labels = tf.placeholder(tf.int32, shape = [None, None])\n",
        "        self.maxlen = tf.shape(self.word_ids)[1]\n",
        "        self.lengths = tf.count_nonzero(self.word_ids, 1)\n",
        "\n",
        "        self.word_embeddings = tf.Variable(\n",
        "            tf.truncated_normal(\n",
        "                [len(word2idx), dim_word], stddev = 1.0 / np.sqrt(dim_word)\n",
        "            )\n",
        "        )\n",
        "        self.char_embeddings = tf.Variable(\n",
        "            tf.truncated_normal(\n",
        "                [len(char2idx), dim_char], stddev = 1.0 / np.sqrt(dim_char)\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        word_embedded = tf.nn.embedding_lookup(\n",
        "            self.word_embeddings, self.word_ids\n",
        "        )\n",
        "        char_embedded = tf.nn.embedding_lookup(\n",
        "            self.char_embeddings, self.char_ids\n",
        "        )\n",
        "        s = tf.shape(char_embedded)\n",
        "        char_embedded = tf.reshape(\n",
        "            char_embedded, shape = [s[0] * s[1], s[-2], dim_char]\n",
        "        )\n",
        "        \n",
        "        for n in range(num_layers):\n",
        "            (out_fw, out_bw), (\n",
        "                state_fw,\n",
        "                state_bw,\n",
        "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw = cells(hidden_size_char),\n",
        "                cell_bw = cells(hidden_size_char),\n",
        "                inputs = char_embedded,\n",
        "                dtype = tf.float32,\n",
        "                scope = 'bidirectional_rnn_char_%d' % (n),\n",
        "            )\n",
        "            char_embedded = tf.concat((out_fw, out_bw), 2)\n",
        "            \n",
        "        output = tf.reshape(\n",
        "            char_embedded[:, -1], shape = [s[0], s[1], 2 * hidden_size_char]\n",
        "        )\n",
        "        word_embedded = tf.concat([word_embedded, output], axis = -1)\n",
        "        for n in range(num_layers):\n",
        "            (out_fw, out_bw), (\n",
        "                state_fw,\n",
        "                state_bw,\n",
        "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw = cells(hidden_size_word),\n",
        "                cell_bw = cells(hidden_size_word),\n",
        "                inputs = word_embedded,\n",
        "                dtype = tf.float32,\n",
        "                scope = 'bidirectional_rnn_word_%d' % (n),\n",
        "            )\n",
        "            word_embedded = tf.concat((out_fw, out_bw), 2)\n",
        "            \n",
        "        logits = tf.layers.dense(word_embedded, len(idx2tag))\n",
        "        y_t = self.labels\n",
        "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n",
        "            logits, y_t, self.lengths\n",
        "        )\n",
        "        self.cost = tf.reduce_mean(-log_likelihood)\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate = learning_rate\n",
        "        ).minimize(self.cost)\n",
        "        mask = tf.sequence_mask(self.lengths, maxlen = self.maxlen)\n",
        "        self.tags_seq, tags_score = tf.contrib.crf.crf_decode(\n",
        "            logits, transition_params, self.lengths\n",
        "        )\n",
        "        self.tags_seq = tf.identity(self.tags_seq, name = 'logits')\n",
        "        \n",
        "        y_t = tf.cast(y_t, tf.int32)\n",
        "        self.prediction = tf.boolean_mask(self.tags_seq, mask)\n",
        "        mask_label = tf.boolean_mask(y_t, mask)\n",
        "        correct_pred = tf.equal(self.prediction, mask_label)\n",
        "        correct_index = tf.cast(correct_pred, tf.float32)\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uSlTFDhSLVD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "e961c76c-e8d6-4ad3-882e-2a54fc01f696"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "dim_word = 128\n",
        "dim_char = 256\n",
        "dropout = 1.0\n",
        "learning_rate = 1e-3\n",
        "hidden_size_char = 64\n",
        "hidden_size_word = 64\n",
        "num_layers = 2\n",
        "batch_size = 64\n",
        "\n",
        "model = Model(dim_word,dim_char,dropout,learning_rate,hidden_size_char,hidden_size_word,num_layers)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "reduction_indices is deprecated, use axis instead\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-22-cb9f809d7691>:17: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-22-cb9f809d7691>:60: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-22-cb9f809d7691>:81: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/crf/python/ops/crf.py:99: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNzm7NKdSNSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "85f9eb09-43ce-4ece-8e75-f5fc2694c869"
      },
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "for e in range(3):\n",
        "    lasttime = time.time()\n",
        "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
        "    pbar = tqdm(\n",
        "        range(0, len(train_X), batch_size), desc = 'train minibatch loop'\n",
        "    )\n",
        "    for i in pbar:\n",
        "        batch_x = train_X[i : min(i + batch_size, train_X.shape[0])]\n",
        "        batch_char = char_train_X[i : min(i + batch_size, train_X.shape[0])]\n",
        "        batch_y = train_Y[i : min(i + batch_size, train_X.shape[0])]\n",
        "        acc, cost, _ = sess.run(\n",
        "            [model.accuracy, model.cost, model.optimizer],\n",
        "            feed_dict = {\n",
        "                model.word_ids: batch_x,\n",
        "                model.char_ids: batch_char,\n",
        "                model.labels: batch_y\n",
        "            },\n",
        "        )\n",
        "        assert not np.isnan(cost)\n",
        "        train_loss += cost\n",
        "        train_acc += acc\n",
        "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
        "    \n",
        "    pbar = tqdm(\n",
        "        range(0, len(test_X), batch_size), desc = 'test minibatch loop'\n",
        "    )\n",
        "    for i in pbar:\n",
        "        batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n",
        "        batch_char = char_test_X[i : min(i + batch_size, test_X.shape[0])]\n",
        "        batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n",
        "        acc, cost = sess.run(\n",
        "            [model.accuracy, model.cost],\n",
        "            feed_dict = {\n",
        "                model.word_ids: batch_x,\n",
        "                model.char_ids: batch_char,\n",
        "                model.labels: batch_y\n",
        "            },\n",
        "        )\n",
        "        assert not np.isnan(cost)\n",
        "        test_loss += cost\n",
        "        test_acc += acc\n",
        "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
        "    \n",
        "    train_loss /= len(train_X) / batch_size\n",
        "    train_acc /= len(train_X) / batch_size\n",
        "    test_loss /= len(test_X) / batch_size\n",
        "    test_acc /= len(test_X) / batch_size\n",
        "\n",
        "    print('time taken:', time.time() - lasttime)\n",
        "    print(\n",
        "        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n",
        "        % (e, train_loss, train_acc, test_loss, test_acc)\n",
        "    )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train minibatch loop: 100%|██████████| 125/125 [01:12<00:00,  1.94it/s, accuracy=0.913, cost=17.4]\n",
            "test minibatch loop: 100%|██████████| 32/32 [00:09<00:00,  3.43it/s, accuracy=0.947, cost=14.8]\n",
            "train minibatch loop:   0%|          | 0/125 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken: 82.04590725898743\n",
            "epoch: 0, training loss: 84.115309, training acc: 0.465857, valid loss: 20.123173, valid acc: 0.941000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train minibatch loop: 100%|██████████| 125/125 [01:11<00:00,  1.86it/s, accuracy=0.998, cost=1.48]\n",
            "test minibatch loop: 100%|██████████| 32/32 [00:09<00:00,  3.51it/s, accuracy=1, cost=1.58]\n",
            "train minibatch loop:   0%|          | 0/125 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken: 80.70353841781616\n",
            "epoch: 1, training loss: 6.858737, training acc: 0.980118, valid loss: 2.518778, valid acc: 1.024097\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train minibatch loop: 100%|██████████| 125/125 [01:10<00:00,  1.89it/s, accuracy=0.998, cost=0.521]\n",
            "test minibatch loop: 100%|██████████| 32/32 [00:09<00:00,  3.44it/s, accuracy=1, cost=0.442]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time taken: 80.36205554008484\n",
            "epoch: 2, training loss: 1.285965, training acc: 1.004070, valid loss: 0.719654, valid acc: 1.029562\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIgJqFaBSRLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            out_i.append(idx2tag[p])\n",
        "        out.append(out_i)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zys8JhMsSUdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e58e3ebc-565b-4c15-eb6f-783e2f492120"
      },
      "source": [
        "real_Y, predict_Y = [], []\n",
        "\n",
        "pbar = tqdm(\n",
        "    range(0, len(test_X), batch_size), desc = 'validation minibatch loop'\n",
        ")\n",
        "for i in pbar:\n",
        "    batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n",
        "    batch_char = char_test_X[i : min(i + batch_size, test_X.shape[0])]\n",
        "    batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n",
        "    predicted = pred2label(sess.run(model.tags_seq,\n",
        "            feed_dict = {\n",
        "                model.word_ids: batch_x,\n",
        "                model.char_ids: batch_char,\n",
        "            },\n",
        "    ))\n",
        "    real = pred2label(batch_y)\n",
        "    predict_Y.extend(predicted)\n",
        "    real_Y.extend(real)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation minibatch loop: 100%|██████████| 32/32 [00:09<00:00,  3.68it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mka0Z5tZScbf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "399e6a3a-474b-46de-b3b9-ff82e09fdc31"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(np.array(real_Y).ravel(), np.array(predict_Y).ravel(), digits = 6))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ   0.998809  0.998333  0.998571      4199\n",
            "         ADP   0.998502  0.999559  0.999030     11339\n",
            "         ADV   0.996052  0.997088  0.996569      4807\n",
            "         AUX   0.986081  1.000000  0.992992       921\n",
            "       CCONJ   0.997556  0.999184  0.998370      3677\n",
            "         DET   0.995592  0.996966  0.996278      3625\n",
            "        NOUN   0.999645  0.998976  0.999311     25403\n",
            "         NUM   0.998997  1.000000  0.999498      3984\n",
            "        PART   0.994444  1.000000  0.997214       537\n",
            "        PRON   0.999798  0.999798  0.999798      4941\n",
            "       PROPN   0.999587  0.999633  0.999610     21776\n",
            "       SCONJ   0.999316  0.993207  0.996252      1472\n",
            "         SYM   1.000000  0.995951  0.997972       247\n",
            "        VERB   0.999436  0.998711  0.999073     12411\n",
            "           X   0.000000  0.000000  0.000000        11\n",
            "\n",
            "    accuracy                       0.998863     99350\n",
            "   macro avg   0.930921  0.931827  0.931369     99350\n",
            "weighted avg   0.998754  0.998863  0.998808     99350\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlxZ0-_0Se7S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8c92fe0-3662-43da-c978-d8966ff936a7"
      },
      "source": [
        "text = 'Husein makan ayam dan ikan'\n",
        "text_sample = np.array([text.split()])\n",
        "char_test = generate_char_seq(text_sample)\n",
        "word_test = word_seq(text_sample)\n",
        "char_test.shape, word_test.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 5, 6), (1, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekT9bB0HSgnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "45f74ad7-9b21-452d-8007-6ddbdad83ef4"
      },
      "source": [
        "char_test"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[49,  5, 10, 13,  9, 11],\n",
              "        [ 0,  3,  2, 25,  2, 11],\n",
              "        [ 0,  0,  2, 24,  2,  3],\n",
              "        [ 0,  0,  0,  7,  2, 11],\n",
              "        [ 0,  0,  9, 25,  2, 11]]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx5uhUH0SiXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7067adf-07ee-4671-9032-2d5da4a24799"
      },
      "source": [
        "predicted = pred2label(sess.run(model.tags_seq,\n",
        "        feed_dict = {\n",
        "            model.word_ids: word_test,\n",
        "            model.char_ids: char_test,\n",
        "        },\n",
        "))\n",
        "predicted"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['PROPN', 'VERB', 'NOUN', 'CCONJ', 'NOUN']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ62774aSkGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8814250-7bb5-4894-cfc7-18e0a10eb3e8"
      },
      "source": [
        "text.split()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Husein', 'makan', 'ayam', 'dan', 'ikan']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}