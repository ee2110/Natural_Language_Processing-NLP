{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Work Process**\n",
    "1. Import dataset and preprocess\n",
    "2. Train model\n",
    "3. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential, layers, losses\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 258s 3us/step\n",
      "84140032/84125825 [==============================] - 258s 3us/step\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view train data files\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unnecessary empty folder\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 10\n",
    "\n",
    "train_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2,\n",
    "    subset='training', \n",
    "    seed=seed)\n",
    "\n",
    "val_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2,\n",
    "    subset='validation', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b\"This film is more about how children make sense of the world around them, and how they (and we) use myth to make sense of it all. I think it's been misperceived, everyone going in expecting a stalkfest won't enjoy it but if you want a deeper story, it's here.......\"\n",
      "0 b'God, I was bored out of my head as I watched this pilot. I had been expecting a lot from it, as I\\'m a huge fan of James Cameron (and not just since \"Titanic\", I might add), and his name in the credits I thought would be a guarantee of quality (Then again, he also wrote the leaden Strange Days..). But the thing failed miserably at grabbing my attention at any point of its almost two hours of duration. In all that time, it barely went beyond its two line synopsis, and I would be very hard pressed to try to figure out any kind of coherent plot out of all the mess of strands that went nowhere. On top of that, I don\\'t think the acrobatics outdid even those of any regular \"A-Team\" episode. As for Alba, yes, she is gorgeous, of course, but the fact that she only displays one single facial expression the entire movie (pouty and surly), makes me also get bored of her \"gal wit an attitude\" schtick pretty soon. You can count me out of this one, Mr. Cameron!'\n",
      "0 b'me, my boyfriend, and our friend watched this \"movie\" if thats what u wanna call it, and we agree with the last person, but we were stupid and bought the damn thing, we thought it really was about diablo so we bought it.<br /><br />we hate it Really SUXZ!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! so beware: DO NOT BUY THIS THING THEY CALL A MOVIE!!!!!!!!!!!!!!!!!!!!!!!<br /><br />we would return it, but don\\'t no if anybody would want this stupid movie.<br /><br />oh and another thing, the shouldn\\'t call it \"The Legend of Diablo\" they should of called it \"Legend of Azar\".<br /><br />and this movie is rated R????? this should not of even been not rated.<br /><br />we think that diablo would be crying his eyes out laughing at this stupid movie.<br /><br />this is a movie that would have been done by a Church.<br /><br />theses \"actors\" are never gonna become nothing because this movie.'\n",
      "0 b\"SPOILERS THROUGHOUT: <br /><br />The Gettaway is mostly an action movie. And what action there is to!! Shootouts, chases, dumpsters and much much more. It stars Kim Bassenger and Alec Baldwin as the Mc Coy's.<br /><br />This is a remake and I have not seen the original but really didn't care for this one at all although Bassenger and Baldwin have some nice screen chemistry. But the movie itself didn't do it for me.<br /><br />The Gettaway became really tiresome really quickly. The plot is overshadowed by one fight/chase after another and as the violence keeps piling up, Bassenger and Baldwin retain their great looks no matter what perils they maybe in. In fact, by the end of the movie they almost look BETTER then in the beginning. I don't think Bassenger's eye makeup moves once during the whole picture.<br /><br />This isn't the worst movie I've ever seen, certainly not, but it isn't very good and unless one is an action movie purist I can't see really enjoying this movie because there's just not a lot here. The Gettaway isn't terribly original either, and goes every way from unnecessarily brutal to rather dull. It really could have been better I think.<br /><br />Bassenger and Baldwin give OK performances but they don't have a lot to do except get chased and run for their lives. Sometimes less is more, after seeing the same thing over and over again it gets stale. Didn't enjoy this one to much.\"\n",
      "0 b'This was a \"cute\" movie at first, then then got too sappy and featured mediocre songs, at best.<br /><br />There is too much King James English spoken with is not only annoying in today\\'s world but not always easy to interpret. Can you imagine young people of today trying to listen to this film? Forget it.<br /><br />Bing Crosby has some good lines in here and is likable as \"Hank Martin.\" Rhonda Fleming (\"Alisande La Carteloise\") was, too, in addition to her good looks and beautiful, long red hair. <br /><br />It\\'s a nice movie with a feel-good ending, and I can\\'t knock that. Maybe this is worthy of a rental, for historical sake or if you\\'re a big Crosby fan but, overall, it\\'s not that much.'\n"
     ]
    }
   ],
   "source": [
    "# sample batch from train data\n",
    "for text_batch, label_batch in train_data.take(1):\n",
    "    \n",
    "    # view the first 5 samples\n",
    "    for i in range(5):\n",
    "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "      lowercase = tf.strings.lower(input_data)\n",
    "      stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "      return tf.strings.regex_replace(stripped_html,\n",
    "                                      '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "                    standardize=custom_standardization,\n",
    "                    max_tokens=vocab_size,\n",
    "                    output_mode='int',\n",
    "                    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(32, activation='relu'),\n",
    "  Dense(1)\n",
    "])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 47s 2s/step - loss: 0.6920 - accuracy: 0.5003 - val_loss: 0.6898 - val_accuracy: 0.4986\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.6863 - accuracy: 0.5003 - val_loss: 0.6818 - val_accuracy: 0.4986\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 4s 198ms/step - loss: 0.6749 - accuracy: 0.5004 - val_loss: 0.6677 - val_accuracy: 0.4986\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 4s 195ms/step - loss: 0.6559 - accuracy: 0.5052 - val_loss: 0.6462 - val_accuracy: 0.5212\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 4s 198ms/step - loss: 0.6286 - accuracy: 0.5525 - val_loss: 0.6175 - val_accuracy: 0.5982\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 4s 197ms/step - loss: 0.5940 - accuracy: 0.6482 - val_loss: 0.5839 - val_accuracy: 0.6822\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 4s 185ms/step - loss: 0.5548 - accuracy: 0.7211 - val_loss: 0.5487 - val_accuracy: 0.7316\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 4s 188ms/step - loss: 0.5145 - accuracy: 0.7621 - val_loss: 0.5152 - val_accuracy: 0.7544\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 4s 186ms/step - loss: 0.4762 - accuracy: 0.7897 - val_loss: 0.4857 - val_accuracy: 0.7698\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 4s 193ms/step - loss: 0.4418 - accuracy: 0.8087 - val_loss: 0.4611 - val_accuracy: 0.7836\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 4s 195ms/step - loss: 0.4115 - accuracy: 0.8239 - val_loss: 0.4411 - val_accuracy: 0.7928\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 4s 196ms/step - loss: 0.3853 - accuracy: 0.8367 - val_loss: 0.4250 - val_accuracy: 0.7992\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 4s 204ms/step - loss: 0.3624 - accuracy: 0.8468 - val_loss: 0.4120 - val_accuracy: 0.8046\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.3422 - accuracy: 0.8565 - val_loss: 0.4018 - val_accuracy: 0.8096\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 4s 194ms/step - loss: 0.3244 - accuracy: 0.8640 - val_loss: 0.3938 - val_accuracy: 0.8144\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 4s 194ms/step - loss: 0.3086 - accuracy: 0.8712 - val_loss: 0.3877 - val_accuracy: 0.8166\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 4s 194ms/step - loss: 0.2945 - accuracy: 0.8773 - val_loss: 0.3832 - val_accuracy: 0.8194\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 4s 195ms/step - loss: 0.2817 - accuracy: 0.8824 - val_loss: 0.3800 - val_accuracy: 0.8228\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 4s 194ms/step - loss: 0.2701 - accuracy: 0.8877 - val_loss: 0.3779 - val_accuracy: 0.8252\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 4s 196ms/step - loss: 0.2595 - accuracy: 0.8931 - val_loss: 0.3767 - val_accuracy: 0.8262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e49879908>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cdd3975230dcdbba\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cdd3975230dcdbba\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the trained word embeddings\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat', 'neg', 'pos', 'urls_neg.txt', 'urls_pos.txt']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view test data files\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b\"An insult to both poker and cinema, this movie manages to make the most dynamic, brilliant, and fascinating figure in poker history into an utter bore. Still a fun film to make jokes about, from the lame gangster movie clich\\xc3\\xa9s of the first half to the incomprehensible nonsense of that second hour. Hilariously, Stu Ungar wins all three of his World Series titles without playing a single hand on screen. His infamous dealer abuse? 1 scene. His coke habit? 1 scene. His incredible memory? 0 scenes. They couldn't even get any real poker players. What did they cover? A lot of high angle shots from inside a house in the suburbs. Oh, and a montage of Stu waking up every day and shopping for meat which doesn't come anywhere close to making sense. Why do I care so much about this little Sopranos summer camp trying to cash in on the poker craze? Because I think there's still a great film to be made about Stu Ungar waiting for someone willing to do it right.\"\n",
      "0 b'(SMALL SPOILERS) I just bought the DVD of this movie yesterday. I saw it with my friends and I couldn\\'t believe what had happened.<br /><br />In the first 3 movies, the critters at least had a sense of humor (especially the 3rd movie), but not only did the critters barely ever make an appearance, they weren\\'t funny! They never made me laugh. I must admit that the story did start off nicely. After an hour had gone by I remembered that the Critters movies were always very short. So I thought to myself, \"Where the $^%#$ are the critters?!?!\" They were barely in this movie! If that didn\\'t make me mad enough, the boy named Ethan was sitting on his bed after Charlie had \"murdered the ship\" and he knew that the critters were still on board! In the first movie the Brown family was scared out of their minds. But here, Ethan didn\\'t even care! It was as if the critters weren\\'t even a threat!<br /><br />Now what I\\'m about to say next may ruin the ending, but I\\'m going to say it anyways. In the first movie, at the end, they had to face the giant critter for a final battle. In the second one, there was the great ball of critter. In the third movie, the critter with his fave burned did a spindash (from Sonic the Hedgehog) and was going to attack the little kid. But at the end of the fourth one (which is what made me the angriest) the bald critter charges toward Ethan, and Ethan kills it as if it were nothing.<br /><br />Now something that I really don\\'t understand was what happened to Ug. He was one of my favorite characters in the first two. Then after 50 years, he\\'s evil. That was very disappointing. Not only that, but wasn\\'t he a faceless bounty hunter? Why was he still \"Johnny Steele?\" Plus he seemed to have a different personality. He seemed much smarter and not as monotone like in the first two.<br /><br />Being someone who actually enjoyed the first two critters movies, and loved the third one, I give Critters 4 a 2/10'\n",
      "0 b\"Very disappointing 7th chapter of this slowly dying series. Very evident that the budget was extremely low. This movie was made for one reason and one reason alone. To sell Puppet Master Toys! Fans, such as myself of the series have decided, from what I have read and heard that the only one in the series worse than this is Curse of the Puppetmaster. In turn, turning us away from the series. <br /><br />Opting to make this a PG-13 film, for whatever reason, did not work in the films favor. The plot seemed almost to be there, but was easily lost in the steady stream of nonsense. <br /><br />The only film in the series worth watching, also directed by Decoteau is part 3 - Toulon's Revenge.<br /><br />Granted, I do favor the scenery in the film. <br /><br />Yuck!\"\n",
      "0 b'Stay away from this movie! It is terrible in every way. Bad acting, a thin recycled plot and the worst ending in film history. Seldom do I watch a movie that makes my adrenaline pump from irritation, in fact the only other movie that immediately springs to mind is another \"people in an aircraft in trouble\" movie (Airspeed). Please, please don\\'t watch this one as it is utterly and totally pathetic from beginning to end. Helge Iversen'\n",
      "0 b\"This film is BORING, BORING, BORING, BORING, and BORING!!! It's not the worse film I ever saw, on the contrary, but.......how shall I put this.......IT'S BORING! There is some very nice scenery and some clever dry wit but that's about it. If it was advertised as a travelogue I would rate it a 7 but it's supposed to be a film with a plot, some drama, and for god's sake a point or a satisfying conclusion.<br /><br />I read some of the comments on this board about this films and I wondered if they saw the same movie as I did.<br /><br />See this film (yawn) at your own risk........one thing for sure- it really is rated correctly= G RATING! (Which most stand for GOD AWFUL BORING!)\"\n"
     ]
    }
   ],
   "source": [
    "# sample batch from test data\n",
    "for test_text_batch, test_label_batch in test_data.take(1):\n",
    "    \n",
    "    # view the first 5 samples\n",
    "    for i in range(5):\n",
    "        print(test_label_batch[i].numpy(), test_text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'This film biography of early rock and roll star Buddy Holly (1936-1959) is a tour de force for Gary Busey. The movie\\'s highlights are Busey\\'s stage performances where he plays guitar and sings Holly songs. He brings such energy to the performances that Holly\\'s own filmed performances almost pale in comparison. Busey\\'s infectious toothy grin lights up the screen, he creates a totally believable and winning personality and his Oscar nomination for best actor was well deserved.<br /><br />The film follows Holly\\'s career from growing up in Lubbock, Texas, to stardom and New York and his untimely death in a plane crash. One thing I found interesting, if true, was Buddy\\'s driving ambition--he had great plans to go beyond recording and performance to producing. As young as he was he was already establishing himself as a shrewd businessman and definitely wanted to take things to a higher level. We will never know if he would have ultimately catapulted his early success into a business brand like The Rolling Stones.<br /><br />The lyrics of many of Holly\\'s songs are pretty adolescent; read the lyrics for \"Peggy Sue\" or \"Oh Boy!\" and you will see what I mean. Maybe to a great extent this explains his popularity with adolescent audiences, but his instrumentation and stage performances surely account for his influence on groups to follow--both The Rolling Stones and The Beatles have acknowledged his importance.<br /><br />Clearly some liberties were taken for dramatic effect. For example, I doubt that Holly ever punched out a producer in Nashville or that the audience at New York\\'s Apollo theater was so immediately responsive as to be wildly dancing in the aisles. If you are interested in getting closer to the truth, see the documentary \"The Real Buddy Holly Story\" (1985) that is produced and hosted by a very relaxed and engaging Paul McCartney. This contains interviews with Holly\\'s family, friends, and band-mates (Holly\\'s musical brothers are not even mentioned in \"The Buddy Holly Story\"). Members of other bands like Keith Richards and Don Everly also offer opinions and stories and there is footage of old Holly performances. The McCartney production can stand on its own, but it makes an excellent companion piece to \"The Buddy Holly Story\" and perhaps should be required viewing for anyone who watches the fictionalized story.', shape=(), dtype=string)\n",
      "Label pos\n",
      "Vectorized review (<tf.Tensor: shape=(1, 100), dtype=int64, numpy=\n",
      "array([[  11,   19, 4980,    5,  410,  860,    4, 2072,  355, 1752, 3086,\n",
      "           1,    7,    3, 2918, 1017, 1079,   16, 1864, 5468,    2,   91,\n",
      "        3255,   23,    1, 1025,  367,  116,   28,  295, 4303,    4, 3209,\n",
      "        3086,  761,   28,  969,  137, 1668,    6,    2,  367,   12,    1,\n",
      "         197,  704,  367,  208, 4786,    8, 1716,    1, 9627,    1, 9236,\n",
      "        2363,   55,    2,  270,   28, 2181,    3,  423,  785,    4, 2238,\n",
      "        1556,    4,   24,  980, 4788,   16,  117,  299,   13,   70, 1875,\n",
      "           2,   19, 1039,    1,  640,   35, 1928,   55,    8,    1, 1709,\n",
      "           6, 6158,    4,  172,  962,    4,   24,    1,  316,    8,    3,\n",
      "        1373]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = next(iter(test_data))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", test_data.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vectorize function is not required to process the test data\n",
    "# if the vectorize layer included in model\n",
    "\n",
    "# test_ds = test_data.map(vectorize_text)\n",
    "\n",
    "# # sample batch from test data\n",
    "# for test_text_batch, test_label_batch in test_ds.take(1):\n",
    "#     for i in range(1):\n",
    "#         print(test_label_batch[i].numpy(), test_text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 26s 34ms/step - loss: 0.4029 - accuracy: 0.8025\n",
      "Loss:  0.40294232964515686\n",
      "Accuracy:  0.8024799823760986\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 20s 24ms/step - loss: 0.9002 - accuracy: 0.5179\n",
      "0.5178800225257874\n"
     ]
    }
   ],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "loss, accuracy = export_model.evaluate(test_data)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, label_batch = next(iter(test_data))\n",
    "first_review, first_label = text_batch[0], label_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = export_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61396766],\n",
       "       [0.45945567],\n",
       "       [0.31570596],\n",
       "       ...,\n",
       "       [0.13894525],\n",
       "       [0.8771089 ],\n",
       "       [0.35029382]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = []\n",
    "\n",
    "for i in range(len(pred_label)):\n",
    "    pred_y.append(round(pred_label[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_y = []\n",
    "for tt, ll in test_data:\n",
    "    for l in ll:\n",
    "        actual_y.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(pred_y)):\n",
    "    if pred_y[i] == actual_y[i]:\n",
    "        correct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.94"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/len(pred_y)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze my own review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_reviews =[\"The new movie is popular and awesome\",\n",
    "             \"The background music is annoying and too loud\",\n",
    "             \"We are very enjoy the movie\",\n",
    "             \"Negative comment in internent is hurt people\",\n",
    "             \"The smile is very sweat and cute!\",\n",
    "             \"The view is so beautiful and attrative\",\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61664504],\n",
       "       [0.58398956],\n",
       "       [0.63175166],\n",
       "       [0.3898934 ],\n",
       "       [0.6680643 ],\n",
       "       [0.5461671 ]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_model.predict(my_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
