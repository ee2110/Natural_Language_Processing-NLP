{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text classification using LSTM RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEdGbUddNZi0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "bca8f5e1-c5d3-4423-e9c6-dc5af0bcb7d3"
      },
      "source": [
        "# Download dataset\n",
        "!wget https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-negative-bm.txt\n",
        "!wget https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-positive-bm.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-11 02:50:22--  https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-negative-bm.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 700425 (684K) [text/plain]\n",
            "Saving to: ‘subjectivity-negative-bm.txt’\n",
            "\n",
            "\r          subjectiv   0%[                    ]       0  --.-KB/s               \rsubjectivity-negati 100%[===================>] 684.01K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-09-11 02:50:22 (10.7 MB/s) - ‘subjectivity-negative-bm.txt’ saved [700425/700425]\n",
            "\n",
            "--2019-09-11 02:50:24--  https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-positive-bm.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 651624 (636K) [text/plain]\n",
            "Saving to: ‘subjectivity-positive-bm.txt’\n",
            "\n",
            "subjectivity-positi 100%[===================>] 636.35K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-09-11 02:50:24 (9.83 MB/s) - ‘subjectivity-positive-bm.txt’ saved [651624/651624]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aleFcSVIOqxB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f862e217-c8f3-4e64-fa7e-f78ad71909b1"
      },
      "source": [
        "# Check if have the dataset downloaded\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  subjectivity-negative-bm.txt  subjectivity-positive-bm.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK0rj3zUOt6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('subjectivity-negative-bm.txt') as fopen:\n",
        "  negative = fopen.read().split('\\n')[:-1]\n",
        "\n",
        "with open('subjectivity-positive-bm.txt') as fopen:\n",
        "  positive = fopen.read().split('\\n')[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUAtSPbKOwsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "167e77fd-478a-46ab-d729-f301b2b8afb6"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 1270525559738965315, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 5407650109671801694\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 12025918761234603702\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11330115994\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 5870716687150579958\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ7V3MMmO-PX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d59d8c17-d373-4bcd-f96a-b3dc20101d7d"
      },
      "source": [
        "# Check the size\n",
        "len(negative), len(positive)\n",
        "\n",
        "# Pre-process data\n",
        "import re\n",
        "\n",
        "def cleaning(string):\n",
        "  string = re.sub('[^A-Za-z ]+', ' ', string.lower())\n",
        "  string = re.sub(r'[ ]+', ' ', string).strip()\n",
        "  return string\n",
        "\n",
        "cleaning(negative[0])\n",
        "\n",
        "\n",
        "for i in range(len(negative)):\n",
        "  negative[i] = cleaning(negative[i])\n",
        "\n",
        "for i in range(len(positive)):\n",
        "  positive[i] = cleaning(positive[i])\n",
        "\n",
        "X = negative + positive\n",
        "Y = [0] * len(negative) + [1] * len(positive)\n",
        "len(X), len(Y)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9960, 9960)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQBV85f2PR6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2819006c-40a3-4629-c351-746d4b9b1816"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'filem bermula pada masa lalu di mana seorang budak lelaki bernama sam cuba menyelamatkan celebi dari pemburu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv71AcmePbKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import json\n",
        "\n",
        "def build_dataset(words, n_words, atleast=1):\n",
        "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
        "    counter = collections.Counter(words).most_common(n_words)\n",
        "    counter = [i for i in counter if i[1] >= atleast]\n",
        "    count.extend(counter)\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return dictionary, reversed_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoHpdKXyRsm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "concat = ' '.join(X).split()\n",
        "size_concat = len(set(concat))\n",
        "dictionary, reversed_dictionary = build_dataset(concat, size_concat, atleast=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scLJ1E5MRv-d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad09953f-a20a-465b-fcd6-29045e135018"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'filem bermula pada masa lalu di mana seorang budak lelaki bernama sam cuba menyelamatkan celebi dari pemburu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FY2BGCLRw6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTjxlAtiR6yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "  def __init__(self, size_layer, num_layers, learning_rate, class_size):\n",
        "    \n",
        "    self.X = tf.placeholder(tf.int32, (None, None))\n",
        "    self.Y = tf.placeholder(tf.int32, (None))\n",
        "    \n",
        "    # word vector definition\n",
        "    embeddings = tf.Variable(tf.random_uniform([len(dictionary), size_layer], -1, 1))\n",
        "    \n",
        "    # lookup batch of integers to word vector\n",
        "    embedded = tf.nn.embedding_lookup(embeddings, self.X)\n",
        "    \n",
        "    def cells(reuse=False):\n",
        "      return tf.nn.rnn_cell.LSTMCell(size_layer,\n",
        "                                     initializer=tf.orthogonal_initializer(),\n",
        "                                     reuse=reuse)\n",
        "    \n",
        "    # in tensorflow, we need to create function that return cell to build multirnn\n",
        "    rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n",
        "    \n",
        "    # output size [B, T, D]\n",
        "    output, _ = tf.nn.dynamic_rnn(rnn_cells, embedded, dtype = tf.float32)\n",
        "    \n",
        "    # logits size [B, D]\n",
        "    self.logits = tf.layers.dense(output[:,-1], class_size)\n",
        "    self.cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits, \n",
        "                                                               labels = self.Y)\n",
        "    self.cost = tf.reduce_mean(self.cost)\n",
        "    \n",
        "    self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
        "    \n",
        "    correct_pred = tf.equal(\n",
        "        tf.argmax(self.logits, 1, output_type = tf.int32), self.Y\n",
        "    )\n",
        "    self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25jTwA5zWHaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "2230851c-9e3d-4d11-e9c6-55be144f7705"
      },
      "source": [
        "size_layer = 256\n",
        "num_layers = 2\n",
        "learning_rate = 1e-3\n",
        "class_size = 2\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "model = Model(size_layer, num_layers, learning_rate, class_size)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-d7db54cb1bad>:16: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-15-d7db54cb1bad>:19: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-15-d7db54cb1bad>:22: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-15-d7db54cb1bad>:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3uWMhZwV7yB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str_idx(corpus, dic, maxlen, UNK = 3):\n",
        "    X = np.zeros((len(corpus), maxlen))\n",
        "    for i in range(len(corpus)):\n",
        "        for no, k in enumerate(corpus[i][:maxlen][::-1]):\n",
        "            X[i, -1 - no] = dic.get(k, UNK)\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOSlfFFAcE0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 100\n",
        "X_idx = str_idx(X, dictionary, maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39E3jZpievao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X_idx, Y, test_size = 0.2)\n",
        "\n",
        "batch_size = 128\n",
        "epoch = 10\n",
        "PAD = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEbhJFQ1exHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21dda663-2c07-46bc-804a-aec0a7c8af49"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "for e in range(epoch):\n",
        "    pbar = tqdm.tqdm(\n",
        "        range(0, len(train_X), batch_size), desc = 'minibatch loop')\n",
        "    train_loss, train_acc, test_loss, test_acc = [], [], [], []\n",
        "    for i in pbar:\n",
        "        index = min(i + batch_size, len(train_X))\n",
        "        batch_x = train_X[i : index]\n",
        "        batch_y = train_Y[i : index]\n",
        "        feed = {model.X: batch_x,\n",
        "                model.Y: batch_y}\n",
        "        accuracy, loss, _ = sess.run([model.accuracy,model.cost,model.optimizer],\n",
        "                                    feed_dict = feed)\n",
        "        train_loss.append(loss)\n",
        "        train_acc.append(accuracy)\n",
        "        pbar.set_postfix(cost = loss, accuracy = accuracy)\n",
        "        \n",
        "    pbar = tqdm.tqdm(\n",
        "        range(0, len(test_X), batch_size), desc = 'minibatch loop')\n",
        "    for i in pbar:\n",
        "        index = min(i + batch_size, len(test_X))\n",
        "        batch_x = test_X[i : index]\n",
        "        batch_y = test_Y[i : index]\n",
        "        feed = {model.X: batch_x,\n",
        "                model.Y: batch_y,}\n",
        "        accuracy, loss = sess.run([model.accuracy,model.cost],\n",
        "                                    feed_dict = feed)\n",
        "\n",
        "        test_loss.append(loss)\n",
        "        test_acc.append(accuracy)\n",
        "        pbar.set_postfix(cost = loss, accuracy = accuracy)\n",
        "    \n",
        "    print('\\nepoch %d, training avg loss %f, training avg acc %f'%(e+1,\n",
        "                                                                 np.mean(train_loss),np.mean(train_acc)))\n",
        "    print('\\nepoch %d, testing avg loss %f, testing avg acc %f'%(e+1,\n",
        "                                                              np.mean(test_loss),np.mean(test_acc)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 63/63 [00:13<00:00,  5.26it/s, accuracy=0.656, cost=0.667]\n",
            "minibatch loop: 100%|██████████| 16/16 [00:01<00:00, 11.52it/s, accuracy=0.472, cost=0.695]\n",
            "minibatch loop:   2%|▏         | 1/63 [00:00<00:11,  5.22it/s, accuracy=0.531, cost=0.697]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1, training avg loss 0.696885, training avg acc 0.517485\n",
            "\n",
            "epoch 1, testing avg loss 0.686804, testing avg acc 0.531467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 63/63 [00:12<00:00,  5.36it/s, accuracy=0.688, cost=0.631]\n",
            "minibatch loop: 100%|██████████| 16/16 [00:01<00:00, 11.92it/s, accuracy=0.653, cost=0.68]\n",
            "minibatch loop:   2%|▏         | 1/63 [00:00<00:11,  5.21it/s, accuracy=0.594, cost=0.699]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2, training avg loss 0.680036, training avg acc 0.568700\n",
            "\n",
            "epoch 2, testing avg loss 0.684244, testing avg acc 0.597439\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 63/63 [00:12<00:00,  5.27it/s, accuracy=0.75, cost=0.64]\n",
            "minibatch loop: 100%|██████████| 16/16 [00:01<00:00, 11.87it/s, accuracy=0.528, cost=0.704]\n",
            "minibatch loop:   2%|▏         | 1/63 [00:00<00:12,  5.14it/s, accuracy=0.578, cost=0.689]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3, training avg loss 0.683671, training avg acc 0.562872\n",
            "\n",
            "epoch 3, testing avg loss 0.681061, testing avg acc 0.559842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 63/63 [00:12<00:00,  5.29it/s, accuracy=0.75, cost=0.541]\n",
            "minibatch loop: 100%|██████████| 16/16 [00:01<00:00, 11.83it/s, accuracy=0.611, cost=0.637]\n",
            "minibatch loop:   2%|▏         | 1/63 [00:00<00:11,  5.24it/s, accuracy=0.609, cost=0.647]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4, training avg loss 0.659359, training avg acc 0.602307\n",
            "\n",
            "epoch 4, testing avg loss 0.629032, testing avg acc 0.647081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 63/63 [00:12<00:00,  5.22it/s, accuracy=0.781, cost=0.489]\n",
            "minibatch loop: 100%|██████████| 16/16 [00:01<00:00, 11.91it/s, accuracy=0.681, cost=0.621]\n",
            "minibatch loop:   0%|          | 0/63 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5, training avg loss 0.604806, training avg acc 0.674355\n",
            "\n",
            "epoch 5, testing avg loss 0.574849, testing avg acc 0.697320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 63/63 [00:12<00:00,  5.11it/s, accuracy=0.812, cost=0.408]\n",
            "minibatch loop: 100%|██████████| 16/16 [00:01<00:00, 11.53it/s, accuracy=0.708, cost=0.541]\n",
            "minibatch loop:   2%|▏         | 1/63 [00:00<00:11,  5.37it/s, accuracy=0.734, cost=0.551]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6, training avg loss 0.546495, training avg acc 0.734251\n",
            "\n",
            "epoch 6, testing avg loss 0.536724, testing avg acc 0.739095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 63/63 [00:12<00:00,  5.17it/s, accuracy=0.781, cost=0.385]\n",
            "minibatch loop: 100%|██████████| 16/16 [00:01<00:00, 11.80it/s, accuracy=0.694, cost=0.534]\n",
            "minibatch loop:   0%|          | 0/63 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7, training avg loss 0.496190, training avg acc 0.763517\n",
            "\n",
            "epoch 7, testing avg loss 0.523402, testing avg acc 0.733344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 63/63 [00:12<00:00,  5.12it/s, accuracy=0.875, cost=0.314]\n",
            "minibatch loop: 100%|██████████| 16/16 [00:01<00:00, 11.78it/s, accuracy=0.792, cost=0.443]\n",
            "minibatch loop:   2%|▏         | 1/63 [00:00<00:11,  5.23it/s, accuracy=0.797, cost=0.471]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8, training avg loss 0.468670, training avg acc 0.788690\n",
            "\n",
            "epoch 8, testing avg loss 0.512206, testing avg acc 0.770671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 63/63 [00:12<00:00,  5.24it/s, accuracy=0.906, cost=0.223]\n",
            "minibatch loop: 100%|██████████| 16/16 [00:01<00:00, 12.08it/s, accuracy=0.806, cost=0.437]\n",
            "minibatch loop:   0%|          | 0/63 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9, training avg loss 0.414847, training avg acc 0.814608\n",
            "\n",
            "epoch 9, testing avg loss 0.504113, testing avg acc 0.780816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "minibatch loop: 100%|██████████| 63/63 [00:12<00:00,  5.24it/s, accuracy=0.938, cost=0.169]\n",
            "minibatch loop: 100%|██████████| 16/16 [00:01<00:00, 11.82it/s, accuracy=0.792, cost=0.468]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10, training avg loss 0.395789, training avg acc 0.823165\n",
            "\n",
            "epoch 10, testing avg loss 0.544042, testing avg acc 0.780924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOQq-t_de3oP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}