{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5187cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a695fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionAISummer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of plain self attention mechanism with einsum operations\n",
    "    Paper: https://arxiv.org/abs/1706.03762\n",
    "    Blog: https://theaisummer.com/transformer/\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: for NLP it is the dimension of the embedding vector\n",
    "            the last dimension size that will be provided in forward(x),\n",
    "            where x is a 3D tensor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # for Step 1\n",
    "        self.to_qvk = nn.Linear(dim, dim * 3, bias=False)\n",
    "        # for Step 2\n",
    "        self.scale_factor = dim ** -0.5  # 1/np.sqrt(dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        assert x.dim() == 3, '3D tensor must be provided'\n",
    "\n",
    "        # Step 1\n",
    "        qkv = self.to_qvk(x)  # [batch, tokens, dim*3 ]\n",
    "\n",
    "        # decomposition to q,v,k\n",
    "        # rearrange tensor to [3, batch, tokens, dim] and cast to tuple\n",
    "        q, k, v = tuple(rearrange(qkv, 'b t (d k) -> k b t d ', k=3))\n",
    "\n",
    "        # Step 2\n",
    "        # Resulting shape: [batch, tokens, tokens]\n",
    "        scaled_dot_prod = torch.einsum('b i d , b j d -> b i j', q, k) * self.scale_factor\n",
    "\n",
    "        if mask is not None:\n",
    "            assert mask.shape == scaled_dot_prod.shape[1:]\n",
    "            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attention = torch.softmax(scaled_dot_prod, dim=-1)\n",
    "\n",
    "        # Step 3\n",
    "        return torch.einsum('b i j , b j d -> b i d', attention, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa1e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionAISummer(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=None):\n",
    "        \"\"\"\n",
    "        Implementation of multi-head attention layer of the original transformer model.\n",
    "        einsum and einops.rearrange is used whenever possible\n",
    "        Args:\n",
    "            dim: token's dimension, i.e. word embedding vector size\n",
    "            heads: the number of distinct representations to learn\n",
    "            dim_head: the dim of the head. In general dim_head<dim.\n",
    "            However, it may not necessary be (dim/heads)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
    "        _dim = self.dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.to_qvk = nn.Linear(dim, _dim * 3, bias=False)\n",
    "        self.W_0 = nn.Linear( _dim, dim, bias=False)\n",
    "        self.scale_factor = self.dim_head ** -0.5\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        assert x.dim() == 3\n",
    "        # Step 1\n",
    "        qkv = self.to_qvk(x)  # [batch, tokens, dim*3*heads ]\n",
    "\n",
    "        # Step 2\n",
    "        # decomposition to q,v,k and cast to tuple\n",
    "        # the resulted shape before casting to tuple will be:\n",
    "        # [3, batch, heads, tokens, dim_head]\n",
    "        q, k, v = tuple(rearrange(qkv, 'b t (d k h) -> k b h t d ', k=3, h=self.heads))\n",
    "\n",
    "        # Step 3\n",
    "        # resulted shape will be: [batch, heads, tokens, tokens]\n",
    "        scaled_dot_prod = torch.einsum('b h i d , b h j d -> b h i j', q, k) * self.scale_factor\n",
    "\n",
    "        if mask is not None:\n",
    "            assert mask.shape == scaled_dot_prod.shape[2:]\n",
    "            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attention = torch.softmax(scaled_dot_prod, dim=-1)\n",
    "\n",
    "        # Step 4. Calc result per batch and per head h\n",
    "        out = torch.einsum('b h i j , b h j d -> b h i d', attention, v)\n",
    "\n",
    "        # Step 5. Re-compose: merge heads with dim_head d\n",
    "        out = rearrange(out, \"b h t d -> b t (h d)\")\n",
    "\n",
    "        # Step 6. Apply final linear transformation layer\n",
    "        return self.W_0(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34983547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .mhsa import MultiHeadSelfAttention\n",
    "\n",
    "\n",
    "class TransformerBlockAISummer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla transformer block from the original paper \"Attention is all you need\"\n",
    "    Detailed analysis: https://theaisummer.com/transformer/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, heads=8, dim_head=None, dim_linear_block=1024, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           dim: token's vector length\n",
    "           heads: number of heads\n",
    "           dim_head: if none dim/heads is used\n",
    "           dim_linear_block: the inner projection dim\n",
    "           dropout: probability of droppping values\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mhsa = MultiHeadSelfAttention(dim=dim, heads=heads, dim_head=dim_head)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.norm_1 = nn.LayerNorm(dim)\n",
    "        self.norm_2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "           nn.Linear(dim, dim_linear_block),\n",
    "           nn.ReLU(),\n",
    "           nn.Dropout(dropout),\n",
    "           nn.Linear(dim_linear_block, dim),\n",
    "           nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        y = self.norm_1(self.drop(self.mhsa(x, mask)) + x)\n",
    "        return self.norm_2(self.linear(y) + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e3704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderAISummer(nn.Module):\n",
    "    def __init__(self, dim, blocks=6, heads=8, dim_head=None):\n",
    "        super().__init__()\n",
    "        self.block_list = [TransformerBlock(dim, heads, dim_head) for _ in range(blocks)]\n",
    "        self.layers = nn.ModuleList(self.block_list)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b892e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
